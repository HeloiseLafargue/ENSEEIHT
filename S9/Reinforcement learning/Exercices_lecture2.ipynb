{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Exercise : Finite horizon MDP\n"],"metadata":{"id":"P8wvIg1ibne8"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","\n","# Paramètres du problème\n","nombre_de_places = 20\n","jours = 50\n","prix1, q1 = 5, 0.1\n","prix2, q2 = 1, 0.8\n","\n","# Création d'une matrice pour stocker les valeurs optimales\n","V = np.zeros((nombre_de_places + 1, jours + 1))\n","\n","# Matrices pour stocker la politique optimale (0 pour p1, 1 pour p2)\n","policy = np.zeros((nombre_de_places + 1, jours + 1), dtype=int)\n","\n","# Remplissage de la matrice de valeur (programmation dynamique)\n","for t in range(jours, -1, -1):\n","    for s in range(nombre_de_places + 1):\n","        if t == jours:\n","            V[s, t] = 0\n","        else:\n","            R1 = prix1 * q1\n","            R2 = prix2 * q2\n","\n","            V1 = R1 + (1 - q1) * V[s, t + 1] + q1 * V[s-1, t+1] if s > 0 else R1\n","            V2 = R2 + (1 - q2) * V[s, t + 1] + q2 * V[s-1, t+1] if s > 0 else R2\n","\n","            V[s, t] = max(V1, V2)\n","            policy[s, t] = 0 if V1 > V2 else 1\n","\n","\n","for t in range(jours, -1, -1):\n","  print(policy[:,t])\n","\n","# Extraction de la stratégie optimale\n","strategie_optimale = np.zeros(jours, dtype=int)\n","places_restantes = nombre_de_places\n","revenu_total = 0\n","place_vendu5 = 0\n","for t in range(jours):\n","    strategie_optimale[t] = policy[places_restantes, t]\n","    if places_restantes > 0:\n","      if strategie_optimale[t] == 0:\n","        if random.random() < q1:\n","          places_restantes -= 1\n","          revenu_total += 5\n","          place_vendu5 += 1\n","      if strategie_optimale[t] == 1:\n","        if random.random() < q2:\n","          places_restantes -= 1\n","          revenu_total += 1\n","print(places_restantes)\n","print(place_vendu5)\n","# Affichage de la stratégie optimale\n","print(\"Stratégie optimale (0 pour p1, 1 pour p2) :\")\n","print(strategie_optimale)\n","\n","\n","print(\"Revenu total avec la stratégie optimale : $\", revenu_total)\n"],"metadata":{"id":"25vOvAHhcEaB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697724936401,"user_tz":-120,"elapsed":5,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"0af64d5d-9181-4629-ee4d-73f9a74f38b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","0\n","1\n","Stratégie optimale (0 pour p1, 1 pour p2) :\n","[0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0\n"," 0 0 0 1 1 1 0 1 1 1 0 1 1]\n","Revenu total avec la stratégie optimale : $ 24\n"]}]},{"cell_type":"markdown","source":["# Exercice Infinite Horizon MDP"],"metadata":{"id":"RiSD1djiitVd"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Paramètres du modèle\n","S = [0, 1]  # ensemble des états\n","A = [0, 1]  # ensemble des actions\n","gamma = 0.1  # facteur de réduction\n","\n","# Fonction de récompense r(s, a)\n","reward = np.array([[1, 10], [-15, 0]])\n","\n","# Algorithme d'itération de la valeur\n","def value_iteration():\n","    V = np.zeros(len(S))\n","    epsilon = 1e-6\n","\n","    while True:\n","        delta = 0\n","\n","        for s in S:\n","            max_value = float('-inf')\n","            for a in A:\n","                new_value = reward[s, a] + gamma * sum([p * V[next_s] for p, next_s in zip([0.5, 0.5], S)])\n","                max_value = max(max_value, new_value)\n","\n","            delta = max(delta, np.abs(V[s] - max_value))\n","            V[s] = max_value\n","\n","        if delta < epsilon:\n","            break\n","\n","    return V\n","\n","optimal_values = value_iteration()\n","print(\"Valeurs optimales pour chaque état:\", optimal_values)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"veWJnMnfidp9","executionInfo":{"status":"ok","timestamp":1698048368196,"user_tz":-120,"elapsed":8,"user":{"displayName":"Léo Meissner","userId":"16741079830641635405"}},"outputId":"8ebca964-a079-43df-e0e8-1bf81cb5a547"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Valeurs optimales pour chaque état: [10.55555553  0.55555555]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Paramètres du modèle\n","S = [0, 1]  # ensemble des états\n","A = [0, 1]  # ensemble des actions\n","gamma = 0.1  # facteur de réduction\n","\n","# Fonction de récompense r(s, a)\n","reward = np.array([[1, 10], [-15, 0]])\n","\n","policy = {0: 0, 1: 1}\n","\n","# Algorithme d'itération de la politique\n","def policy_iteration(policy):\n","    epsilon = 1e-6\n","\n","    while True:\n","        # Étape d'évaluation de la politique\n","        V = np.zeros(len(S))\n","        while True:\n","            delta = 0\n","\n","            for s in S:\n","                a = policy[s]\n","\n","                # Calcul de la nouvelle valeur pour l'état s et l'action a\n","                new_value = reward[s, a] + gamma * V[s]\n","\n","                delta = max(delta, np.abs(V[s] - new_value))\n","                V[s] = new_value\n","\n","            if delta < epsilon:\n","                break\n","\n","        # Étape d'amélioration de la politique\n","        policy_stable = True\n","        for s in S:\n","            old_action = policy[s]\n","            policy[s] = np.argmax([reward[s, a] + gamma * sum([p * V[next_s] for p, next_s in zip([0.5, 0.5], S)]) for a in A])\n","\n","            if old_action != policy[s]:\n","                policy_stable = False\n","\n","        if policy_stable:\n","            return policy\n","\n","optimal_policy = policy_iteration(policy)\n","print(\"Politique optimale:\", optimal_policy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOZXnR24kwzX","executionInfo":{"status":"ok","timestamp":1698048767297,"user_tz":-120,"elapsed":8,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"942837c8-5b31-4e7d-d414-ee47992d874b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Politique optimale: {0: 1, 1: 1}\n"]}]}]}