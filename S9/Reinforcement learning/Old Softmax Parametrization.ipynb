{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOml2GOVPGf3ZVLqKJY3Etu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Old Softmax Parametrization"],"metadata":{"id":"vVJWz04xI32c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6mHN9FjI1O5"},"outputs":[],"source":["# Softmax Policy Parametrization\n","class SoftmaxPolicy:\n","    def __init__(self, num_actions, num_features):\n","        self.num_actions = num_actions\n","        self.num_features = num_features\n","        self.theta = np.zeros((num_features, num_actions))  # Initialize policy parameters\n","\n","    # Softmax action selection\n","    def select_action(self, state):\n","        action_probs = self.calculate_softmax_probabilities(state)\n","        chosen_action = np.random.choice(np.arange(self.num_actions), p=action_probs)\n","        if chosen_action == 0:\n","          return \"a1\"\n","        else :\n","          return \"a2\"\n","\n","    # Calculate softmax probabilities\n","    def calculate_softmax_probabilities(self, state):\n","        exp_values = np.exp(np.dot(self.theta.T, state))\n","        return exp_values / np.sum(exp_values)\n","\n","    # Gradient of log(πθ(a|s))\n","    def gradient_log_pi_theta(self, state, action, action_probs):\n","        grad_log_pi = np.zeros((self.num_features, self.num_actions))\n","        if action == \"a1\":\n","          action = 0\n","        else :\n","          action = 1\n","        grad_log_pi[:, action] = state - np.dot(state, action_probs)\n","        return grad_log_pi\n"]},{"cell_type":"code","source":["# Initialize policy parameters\n","theta = np.random.rand(2,2)\n","alpha = 0.01  # Learning rate\n","gamma = 0.99  # Discount factor\n","num_episodes = 1000\n","env = ServerEnvironment()\n","policy = SoftmaxPolicy(2, 2)  # Assuming num_features and num_actions are defined properly\n","# Initialiser les listes pour stocker les récompenses\n","total_rewards = []\n","average_rewards = []\n","\n","# Reinforcement learning loop\n","for episode in range(num_episodes):\n","    # Reset environment for each episode\n","    env.reset()\n","    state = env.get_state()\n","    trajectory = []  # Store (state, action, reward) tuples\n","    total_reward = 0  # Initialiser la récompense totale pour cet épisode\n","\n","    compteur = 0\n","    while compteur < 100:\n","        action = policy.select_action(state)  # Select action using policy\n","        next_state, reward, _ = env.step(action)  # Take action and observe next state, reward, and done flag\n","        trajectory.append((state, action, reward))  # Store (state, action, reward) tuple in trajectory\n","        state = next_state  # Move to next state\n","        compteur += 1\n","\n","    # Calculate returns (Gt) for each time step using the rewards\n","    Gt_list = []\n","    Gt = 0\n","    for t in reversed(range(len(trajectory))):\n","        _, _, reward = trajectory[t]\n","        Gt = gamma * Gt + reward\n","        Gt_list.insert(0, Gt)\n","        total_reward += reward\n","\n","    total_rewards.append(total_reward)\n","    average_reward = np.mean(total_rewards) if len(total_rewards) > 0 else 0\n","    average_rewards.append(average_reward)\n","\n","    # Update policy parameters using the gradient\n","    for t, (state, action, _) in enumerate(trajectory):\n","\n","        # Calculate gradient of log probability\n","        action_probs = policy.calculate_softmax_probabilities(state)\n","        log_prob_gradient = policy.gradient_log_pi_theta(state, action, action_probs)\n","\n","        # Update policy parameters using the REINFORCE update rule\n","        theta += alpha * log_prob_gradient * Gt_list[t] # θt+1 = θt + α * gamma^t * ∇θ log(πθ(At|St))Gt\n","\n","# Représentation de l'action optimale en fonction de l'état (Q1, Q2)\n","actions_optimales = np.zeros((max_Q + 1, max_Q + 1))\n","for Q1 in range(max_Q + 1):\n","    for Q2 in range(max_Q + 1):\n","        actions_optimales[Q1, Q2] = max(policy.calculate_softmax_probabilities((Q1, Q2)))\n","\n","# Tracer le total des récompenses au fil du temps\n","plt.figure(figsize=(10, 6))\n","plt.plot(total_rewards)\n","plt.title('Total des récompenses par épisode')\n","plt.xlabel('Épisode')\n","plt.ylabel('Total des récompenses')\n","plt.show()\n","\n","# Tracer la récompense moyenne au fil du temps\n","plt.figure(figsize=(10, 6))\n","plt.plot(average_rewards)\n","plt.title('Récompense moyenne par épisode')\n","plt.xlabel('Épisode')\n","plt.ylabel('Récompense moyenne')\n","plt.show()\n","\n","# Affichage de l'action optimale en fonction de l'état (Q1, Q2)\n","plt.figure(figsize=(10, 8))\n","plt.imshow(actions_optimales, interpolation='nearest')\n","plt.colorbar(label='Action optimale (0 pour a1, 1 pour a2)')\n","plt.title('Action Optimale en fonction de l\\'État (Q1, Q2)')\n","plt.xlabel('Q2 (Nombre de travaux dans le Serveur 2)')\n","plt.ylabel('Q1 (Nombre de travaux dans le Serveur 1)')\n","plt.gca().invert_yaxis()  # Inversion de l'axe des ordonnées\n","plt.show()\n"],"metadata":{"id":"pUUBxfHiI3JX"},"execution_count":null,"outputs":[]}]}