{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"83e22415c1374326ae28cadc3068096a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9b09f13e28c4377857b47bf5d8314b9","IPY_MODEL_d51c22233c9247de9b6be327d80abad0","IPY_MODEL_c9f250efdf4a4795abc080b7f8b232ff"],"layout":"IPY_MODEL_008caf4b73b34a0fb3671960f385ad77"}},"a9b09f13e28c4377857b47bf5d8314b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88dd5cc3e1504ee7a8a0e2d9e56255f0","placeholder":"​","style":"IPY_MODEL_e81c37d8cc6a4576b0c96d5840b74b81","value":"100%"}},"d51c22233c9247de9b6be327d80abad0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_273f2acd7b9a4a5eaea9199f5591a9d4","max":18983,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ce7b8fed99d43a8a9a4c7c1e51a47a2","value":18983}},"c9f250efdf4a4795abc080b7f8b232ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c325615e3d7d4449a5112e786165a7a1","placeholder":"​","style":"IPY_MODEL_ae69c254f691432eaaa0783679d7b714","value":" 18983/18983 [00:15&lt;00:00, 1107.93it/s]"}},"008caf4b73b34a0fb3671960f385ad77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88dd5cc3e1504ee7a8a0e2d9e56255f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e81c37d8cc6a4576b0c96d5840b74b81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"273f2acd7b9a4a5eaea9199f5591a9d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ce7b8fed99d43a8a9a4c7c1e51a47a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c325615e3d7d4449a5112e786165a7a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae69c254f691432eaaa0783679d7b714":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ls4hgfTEHgGR"},"source":["# Réseaux Génératifs Antagonistes\n","\n","Dans ce TP nous allons mettre en place l'entraînement d'un réseau de neurone génératif, entraîné de manière antagoniste à l'aide d'un réseau discriminateur.\n","\n","<center> <img src=\"https://drive.google.com/uc?id=1_ADmA-Js37z6R-0o476dzX4jMG5WHLtr\" width=600></center>\n","<caption><center> Schéma global de fonctionnement d'un GAN ([Goodfellow 2014]) </center></caption>\n","\n","Dans un premier temps, nous allons illustrer le fonctionnement du GAN sur l'exemple simple, canonique, de la base de données MNIST.\n","Votre objectif sera par la suite d'adapter cet exemple à la base de données *Labelled Faces in the Wild*, et éventuellement d'implémenter quelques astuces permettant d'améliorer l'entrainement.\n"]},{"cell_type":"code","metadata":{"id":"TRziuDJMInpM","executionInfo":{"status":"ok","timestamp":1700215232985,"user_tz":-60,"elapsed":3840,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFNzLxouIfwy"},"source":["On commence par définir les réseaux discriminateur et générateur, en suivant les recommandations de DCGAN (activation *LeakyReLU*, *stride*, *Batch Normalization*, activation de sortie *tanh* pour le générateur)"]},{"cell_type":"code","metadata":{"id":"IfPhxKGLHfD-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700127406920,"user_tz":-60,"elapsed":4508,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"cff7de12-801c-4549-d06f-adce246e00e5"},"source":["latent_dim = 128\n","discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(28, 28, 1)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.GlobalMaxPooling2D(),\n","        layers.Dense(1, activation=\"sigmoid\"),\n","    ],\n","    name=\"discriminator\",\n",")\n","discriminator.summary()\n","\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        layers.Dense(7 * 7 * 128),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Reshape((7, 7, 128)),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"tanh\"),\n","    ],\n","    name=\"generator\",\n",")\n","generator.summary()"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 14, 14, 64)        640       \n","                                                                 \n"," batch_normalization (Batch  (None, 14, 14, 64)        256       \n"," Normalization)                                                  \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 14, 14, 64)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 7, 7, 128)         73856     \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 7, 7, 128)         512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n","                                                                 \n"," global_max_pooling2d (Glob  (None, 128)               0         \n"," alMaxPooling2D)                                                 \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 75393 (294.50 KB)\n","Trainable params: 75009 (293.00 KB)\n","Non-trainable params: 384 (1.50 KB)\n","_________________________________________________________________\n","Model: \"generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_1 (Dense)             (None, 6272)              809088    \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 6272)              25088     \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 6272)              0         \n","                                                                 \n"," reshape (Reshape)           (None, 7, 7, 128)         0         \n","                                                                 \n"," conv2d_transpose (Conv2DTr  (None, 14, 14, 128)       262272    \n"," anspose)                                                        \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 14, 14, 128)       512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 128)       0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2D  (None, 28, 28, 128)       262272    \n"," Transpose)                                                      \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 28, 28, 128)       512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 28, 28, 128)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 28, 28, 1)         6273      \n","                                                                 \n","=================================================================\n","Total params: 1366017 (5.21 MB)\n","Trainable params: 1352961 (5.16 MB)\n","Non-trainable params: 13056 (51.00 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"kZ0FTcu6yl56"},"source":["Le code suivant décrit ce qui se passe à chaque itération de l'algorithme, ce qui est également résumé dans le cours sur le slide suivant :\n","\n","<center> <img src=\"https://drive.google.com/uc?id=16FmBkkbWW1HWIwBoG_Sdqq524fFT1sIS\" width=600></center>\n"]},{"cell_type":"code","metadata":{"id":"_RnxhJX_KJxF","executionInfo":{"status":"ok","timestamp":1700128888964,"user_tz":-60,"elapsed":3,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}}},"source":["# Instanciation de deux optimiseurs, l'un pour le discrimnateur et l'autre pour le générateur\n","d_optimizer = keras.optimizers.Adam(learning_rate=0.0008)\n","g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)\n","\n","# Instanciation d'une fonction de coût entropie croisée\n","loss_fn = keras.losses.BinaryCrossentropy()\n","\n","\n","# La fonction prend en entrée un mini-batch d'images réelles\n","@tf.function\n","def train_step(real_images):\n","    # ENTRAINEMENT DU DISCRIMINATEUR\n","    # Échantillonnage d’un mini-batch de bruit\n","    # Remplissage des dimensions manquantes\n","    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","\n","    # Création d'un mini-batch d'images générées à partir du bruit\n","    generated_images = generator(random_latent_vectors)\n","    # Échantillonnage d’un mini-batch de données combinant images générées et réelles\n","    combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","    # Création des labels associés au mini-batch de données créé précédemment\n","    # Pour l'entraînement du discriminateur :\n","    #   - les données générées sont labellisées \"0\"\n","    #   - les données réelles sont labellisées \"1\"\n","    labels = tf.concat([tf.zeros((batch_size, 1)), tf.ones((real_images.shape[0], 1))], axis=0)\n","\n","    # Entraînement du discriminateur\n","    with tf.GradientTape() as tape:\n","        # L'appel d'un modèle (ici discriminator) à l'intérieur de Tf.GradientTape\n","        # permet de récupérer les gradients pour faire la mise à jour\n","\n","        # Prédiction du discriminateur sur notre batch d'images réelles et générées\n","        predictions = discriminator(combined_images)\n","\n","        # Calcul de la fonction de coût\n","        d_loss = loss_fn(labels, predictions)\n","\n","    # Récupération des gradients de la fonction de coût par rapport aux paramètres du discriminateur\n","    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n","    # Mise à jour des paramètres par l'optimiseur grâce aux gradients de la fonction de coût\n","    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n","    ### NOTE : ON N'ENTRAINE PAS LE GENERATEUR A CE MOMENT !\n","\n","    # ENTRAINEMENT DU GENERATEUR\n","    # Échantillonnage d’un mini-batch de bruit\n","    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","    # Création des labels associés au mini-batch de données créé précédemment\n","    # Pour l'entraînement du générateur :\n","    #   - les données générées sont labellisées ici \"1\"\n","    misleading_labels =  tf.ones((batch_size, 1))\n","\n","    # Entraînement du générateur sans toucher aux paramètres du discriminateur !\n","    with tf.GradientTape() as tape:\n","        predictions = discriminator(generator(random_latent_vectors))\n","        g_loss = loss_fn(misleading_labels, predictions)\n","\n","    # Récupération des gradients de la fonction de coût par rapport aux paramètres du générateur\n","    grads = tape.gradient(g_loss, generator.trainable_weights)\n","    # Mise à jour des paramètres par l'optimiseur grâce aux gradients de la fonction de coût\n","    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n","\n","    return d_loss, g_loss, generated_images"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"all1LAF92h1u"},"source":["Il reste à écrire l'algorithme final qui va faire appel au code d'itération écrit précédemment"]},{"cell_type":"code","metadata":{"id":"lQJWoazN2pwd","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"15q_KhZL4xeGSdd0rb91kG8yeSEMR7r_o"},"executionInfo":{"status":"ok","timestamp":1700129737431,"user_tz":-60,"elapsed":846543,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"df35fcf3-762f-4a60-ec10-c6495e0e01b8"},"source":["# Préparation de la base de données : on utilise toutes les images (entraînement + test) de MNIST\n","batch_size = 32\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","all_digits = np.concatenate([x_train, x_test])\n","all_digits = (all_digits.astype(\"float32\")-127.5) / 127.5 # Images normalisées\n","all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","epochs = 20  # Une 20aine d'epochs est nécessaire pour voir des chiffres qui semblent réalistes\n","\n","for epoch in range(epochs):\n","    print(\"\\nStart epoch\", epoch)\n","\n","    for step, real_images in enumerate(dataset):\n","        # Descente de gradient simultanée du discrimnateur et du générateur\n","        d_loss, g_loss, generated_images = train_step(real_images)\n","\n","        # Affichage régulier d'images générées.\n","        if step % 200 == 0:\n","            # Métriques\n","            print(\"Perte du discriminateur à l'étape %d: %.2f\" % (step, d_loss))\n","            print(\"Perte du générateur à l'étape %d: %.2f\" % (step, g_loss))\n","\n","            plt.figure(figsize=(20, 4))\n","            for i in range(10):\n","              plt.subplot(1,10, i+1)\n","              plt.imshow(generated_images[i, :, :, 0]*128+128, cmap='gray')\n","\n","            plt.show()\n"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"kwIc9354oNIV"},"source":["# Travail à faire :\n","\n","Prenez le temps de lire, de comprendre et de compléter le code qui vous est fourni. Observez attentivement l'évolution des métriques ainsi que les images générées au cours de l'entraînement. L'objectif de ce TP est d'abord de vous fournir un exemple de code implémentant les GANs, mais surtout de vous faire sentir la difficulté d'entraîner ces modèles.\n","\n","Dans la suite du TP, nous vous fournissons ci-dessous un code de chargement de la base de données de visages *Labelled Faces in the Wild*. Votre objectif est donc d'adapter le code précédent pour générer non plus des chiffres mais des visages.\n","\n","Quelques précisions importantes, et indications :\n","\n","\n","*   MNIST est une base de données d'images noir et blanc de dimension 28 $\\times$ 28, LFW est une base de données d'images couleur de dimension 32 $\\times$ 32 $\\times$ 3\n","*   La diversité des visages est bien plus grande que celle des chiffres ; votre générateur doit donc être un peu plus complexe que celui utilisé ici (plus de couches, et/ou plus de filtres par exemple)\n","*   Pour faire fonctionner ce second exemple, il pourrait être nécessaire de modifier quelques hyperparamètres (dimension de l'espace latent, taux d'apprentissage des générateur et discriminateur, etc.)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ohexDvCYrahC"},"source":["Le code suivant télécharge et prépare les données de la base LFW."]},{"cell_type":"code","metadata":{"id":"Ot-zkfDBQUkl","colab":{"base_uri":"https://localhost:8080/","height":757,"referenced_widgets":["83e22415c1374326ae28cadc3068096a","a9b09f13e28c4377857b47bf5d8314b9","d51c22233c9247de9b6be327d80abad0","c9f250efdf4a4795abc080b7f8b232ff","008caf4b73b34a0fb3671960f385ad77","88dd5cc3e1504ee7a8a0e2d9e56255f0","e81c37d8cc6a4576b0c96d5840b74b81","273f2acd7b9a4a5eaea9199f5591a9d4","3ce7b8fed99d43a8a9a4c7c1e51a47a2","c325615e3d7d4449a5112e786165a7a1","ae69c254f691432eaaa0783679d7b714"]},"executionInfo":{"status":"ok","timestamp":1700215427934,"user_tz":-60,"elapsed":194956,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"9c8d9689-0331-4666-90c8-5341ef8b4d7a"},"source":["import pandas as pd\n","import tarfile, tqdm, cv2, os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Télécharger les données de la base de données \"Labelled Faces in the Wild\"\n","!wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n","!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n","!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n","\n","ATTRS_NAME = \"lfw_attributes.txt\"\n","IMAGES_NAME = \"lfw-deepfunneled.tgz\"\n","RAW_IMAGES_NAME = \"lfw.tgz\"\n","\n","def decode_image_from_raw_bytes(raw_bytes):\n","    img = cv2.imdecode(np.asarray(bytearray(raw_bytes), dtype=np.uint8), 1)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    return img\n","\n","def load_lfw_dataset(\n","        use_raw=False,\n","        dx=80, dy=80,\n","        dimx=45, dimy=45):\n","\n","    # Read attrs\n","    df_attrs = pd.read_csv(ATTRS_NAME, sep='\\t', skiprows=1)\n","    df_attrs = pd.DataFrame(df_attrs.iloc[:, :-1].values, columns=df_attrs.columns[1:])\n","    imgs_with_attrs = set(map(tuple, df_attrs[[\"person\", \"imagenum\"]].values))\n","\n","    # Read photos\n","    all_photos = []\n","    photo_ids = []\n","\n","    # tqdm in used to show progress bar while reading the data in a notebook here, you can change\n","    # tqdm_notebook to use it outside a notebook\n","    with tarfile.open(RAW_IMAGES_NAME if use_raw else IMAGES_NAME) as f:\n","        for m in tqdm.tqdm_notebook(f.getmembers()):\n","            # Only process image files from the compressed data\n","            if m.isfile() and m.name.endswith(\".jpg\"):\n","                # Prepare image\n","                img = decode_image_from_raw_bytes(f.extractfile(m).read())\n","\n","                # Crop only faces and resize it\n","                img = img[dy:-dy, dx:-dx]\n","                img = cv2.resize(img, (dimx, dimy))\n","\n","                # Parse person and append it to the collected data\n","                fname = os.path.split(m.name)[-1]\n","                fname_splitted = fname[:-4].replace('_', ' ').split()\n","                person_id = ' '.join(fname_splitted[:-1])\n","                photo_number = int(fname_splitted[-1])\n","                if (person_id, photo_number) in imgs_with_attrs:\n","                    all_photos.append(img)\n","                    photo_ids.append({'person': person_id, 'imagenum': photo_number})\n","\n","    photo_ids = pd.DataFrame(photo_ids)\n","    all_photos = np.stack(all_photos).astype('uint8')\n","\n","    # Preserve photo_ids order!\n","    all_attrs = photo_ids.merge(df_attrs, on=('person', 'imagenum')).drop([\"person\", \"imagenum\"], axis=1)\n","\n","    return all_photos, all_attrs\n","\n","# Prépare le dataset et le charge dans la variable X\n","X, attr = load_lfw_dataset(use_raw=True, dimx=32, dimy=32)\n","# Normalise les images\n","X = (X.astype(\"float32\")-127.5)/127.5\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-11-17 10:00:33--  http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n","Resolving www.cs.columbia.edu (www.cs.columbia.edu)... 128.59.11.206\n","Connecting to www.cs.columbia.edu (www.cs.columbia.edu)|128.59.11.206|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt [following]\n","--2023-11-17 10:00:33--  https://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n","Connecting to www.cs.columbia.edu (www.cs.columbia.edu)|128.59.11.206|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14879205 (14M) [text/plain]\n","Saving to: ‘lfw_attributes.txt’\n","\n","lfw_attributes.txt  100%[===================>]  14.19M  36.2MB/s    in 0.4s    \n","\n","2023-11-17 10:00:33 (36.2 MB/s) - ‘lfw_attributes.txt’ saved [14879205/14879205]\n","\n","--2023-11-17 10:00:33--  http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n","Resolving vis-www.cs.umass.edu (vis-www.cs.umass.edu)... 128.119.244.95\n","Connecting to vis-www.cs.umass.edu (vis-www.cs.umass.edu)|128.119.244.95|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 108761145 (104M) [application/x-gzip]\n","Saving to: ‘lfw-deepfunneled.tgz’\n","\n","lfw-deepfunneled.tg 100%[===================>] 103.72M  1.33MB/s    in 72s     \n","\n","2023-11-17 10:01:46 (1.43 MB/s) - ‘lfw-deepfunneled.tgz’ saved [108761145/108761145]\n","\n","--2023-11-17 10:01:46--  http://vis-www.cs.umass.edu/lfw/lfw.tgz\n","Resolving vis-www.cs.umass.edu (vis-www.cs.umass.edu)... 128.119.244.95\n","Connecting to vis-www.cs.umass.edu (vis-www.cs.umass.edu)|128.119.244.95|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 180566744 (172M) [application/x-gzip]\n","Saving to: ‘lfw.tgz’\n","\n","lfw.tgz             100%[===================>] 172.20M  1.39MB/s    in 1m 41s  \n","\n","2023-11-17 10:03:28 (1.70 MB/s) - ‘lfw.tgz’ saved [180566744/180566744]\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-ab8eea6dbdd0>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for m in tqdm.tqdm_notebook(f.getmembers()):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/18983 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83e22415c1374326ae28cadc3068096a"}},"metadata":{}}]},{"cell_type":"code","source":["latent_dim = 256\n","discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(32, 32, 3)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        #layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        #layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        #layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n","        #layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        #layers.GlobalMaxPooling2D(),\n","        layers.Flatten(),\n","        layers.Dropout(0.4),\n","        layers.Dense(1, activation=\"sigmoid\"),\n","    ],\n","    name=\"discriminator\",\n",")\n","discriminator.summary()\n","\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        layers.Dense(4 * 4 * 256),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Reshape((4, 4, 256)),\n","        layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.BatchNormalization(momentum = 0.8),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(3, (7, 7), padding=\"same\", activation=\"tanh\"),\n","    ],\n","    name=\"generator\",\n",")\n","generator.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdJj4wwGpWRA","executionInfo":{"status":"ok","timestamp":1700215431643,"user_tz":-60,"elapsed":3753,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"6f2d6795-3789-4f1b-aeaf-15575a0a2dad"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 16, 16, 64)        1792      \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 8, 8, 128)         73856     \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 4, 4, 128)         147584    \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 128)         0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 2, 2, 256)         295168    \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 256)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 1025      \n","                                                                 \n","=================================================================\n","Total params: 519425 (1.98 MB)\n","Trainable params: 519425 (1.98 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Model: \"generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_1 (Dense)             (None, 4096)              1052672   \n","                                                                 \n"," batch_normalization (Batch  (None, 4096)              16384     \n"," Normalization)                                                  \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 4096)              0         \n","                                                                 \n"," reshape (Reshape)           (None, 4, 4, 256)         0         \n","                                                                 \n"," conv2d_transpose (Conv2DTr  (None, 8, 8, 128)         295040    \n"," anspose)                                                        \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 8, 8, 128)         512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 128)         0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2D  (None, 16, 16, 128)       147584    \n"," Transpose)                                                      \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 16, 16, 128)       512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 128)       0         \n","                                                                 \n"," conv2d_transpose_2 (Conv2D  (None, 32, 32, 128)       147584    \n"," Transpose)                                                      \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 32, 32, 128)       512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_7 (LeakyReLU)   (None, 32, 32, 128)       0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 32, 32, 3)         18819     \n","                                                                 \n","=================================================================\n","Total params: 1679619 (6.41 MB)\n","Trainable params: 1670659 (6.37 MB)\n","Non-trainable params: 8960 (35.00 KB)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Instanciation de deux optimiseurs, l'un pour le discrimnateur et l'autre pour le générateur\n","d_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n","g_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n","\n","# Instanciation d'une fonction de coût entropie croisée\n","loss_fn = keras.losses.BinaryCrossentropy()\n","\n","\n","# La fonction prend en entrée un mini-batch d'images réelles\n","@tf.function\n","def train_step(real_images):\n","    # ENTRAINEMENT DU DISCRIMINATEUR\n","    # Échantillonnage d’un mini-batch de bruit\n","    # Remplissage des dimensions manquantes\n","    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","\n","    # Création d'un mini-batch d'images générées à partir du bruit\n","    generated_images = generator(random_latent_vectors)\n","    # Échantillonnage d’un mini-batch de données combinant images générées et réelles\n","    combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","    # Création des labels associés au mini-batch de données créé précédemment\n","    # Pour l'entraînement du discriminateur :\n","    #   - les données générées sont labellisées \"0\"\n","    #   - les données réelles sont labellisées \"1\"\n","    labels = tf.concat([tf.zeros((batch_size, 1)), tf.ones((real_images.shape[0], 1))], axis=0)\n","\n","    # Entraînement du discriminateur\n","    with tf.GradientTape() as tape:\n","        # L'appel d'un modèle (ici discriminator) à l'intérieur de Tf.GradientTape\n","        # permet de récupérer les gradients pour faire la mise à jour\n","\n","        # Prédiction du discriminateur sur notre batch d'images réelles et générées\n","        predictions = discriminator(combined_images)\n","\n","        # Calcul de la fonction de coût\n","        d_loss = loss_fn(labels, predictions)\n","\n","    # Récupération des gradients de la fonction de coût par rapport aux paramètres du discriminateur\n","    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n","    # Mise à jour des paramètres par l'optimiseur grâce aux gradients de la fonction de coût\n","    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n","    ### NOTE : ON N'ENTRAINE PAS LE GENERATEUR A CE MOMENT !\n","\n","    # ENTRAINEMENT DU GENERATEUR\n","    # Échantillonnage d’un mini-batch de bruit\n","    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","    # Création des labels associés au mini-batch de données créé précédemment\n","    # Pour l'entraînement du générateur :\n","    #   - les données générées sont labellisées ici \"1\"\n","    misleading_labels =  tf.ones((batch_size, 1))\n","\n","    # Entraînement du générateur sans toucher aux paramètres du discriminateur !\n","    with tf.GradientTape() as tape:\n","        predictions = discriminator(generator(random_latent_vectors))\n","        g_loss = loss_fn(misleading_labels, predictions)\n","\n","    # Récupération des gradients de la fonction de coût par rapport aux paramètres du générateur\n","    grads = tape.gradient(g_loss, generator.trainable_weights)\n","    # Mise à jour des paramètres par l'optimiseur grâce aux gradients de la fonction de coût\n","    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n","\n","    return d_loss, g_loss, generated_images"],"metadata":{"id":"UcXpkyrepeDj","executionInfo":{"status":"ok","timestamp":1700215431645,"user_tz":-60,"elapsed":25,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Préparation de la base de données : on utilise toutes les images (entraînement + test) de MNIST\n","batch_size = 32\n","all_digits = np.reshape(X, (-1, 32, 32, 3))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","epochs = 20  # Une 20aine d'epochs est nécessaire pour voir des chiffres qui semblent réalistes\n","\n","for epoch in range(epochs):\n","    print(\"\\nStart epoch\", epoch)\n","\n","    for step, real_images in enumerate(dataset):\n","        # Descente de gradient simultanée du discrimnateur et du générateur\n","        d_loss, g_loss, generated_images = train_step(real_images)\n","\n","        # Affichage régulier d'images générées.\n","        if step % 200 == 0:\n","            # Métriques\n","            print(\"Perte du discriminateur à l'étape %d: %.2f\" % (step, d_loss))\n","            print(\"Perte du générateur à l'étape %d: %.2f\" % (step, g_loss))\n","\n","            plt.figure(figsize=(20, 4))\n","            for i in range(10):\n","              plt.subplot(1,10, i+1)\n","              plt.imshow((generated_images[i, :, :, :]*127.5+127.5).numpy().astype('uint8'))\n","\n","            plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1GJSu7LbG2Xc62Lh31ACKdXL1sNziVMEa"},"id":"dlGUOcdPlb7_","executionInfo":{"status":"ok","timestamp":1700215676701,"user_tz":-60,"elapsed":245074,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"8a16c6f8-2b43-47bb-d48b-a510126924e2"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}