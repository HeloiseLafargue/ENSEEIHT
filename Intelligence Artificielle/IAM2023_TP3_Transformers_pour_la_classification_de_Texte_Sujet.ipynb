{"cells":[{"cell_type":"markdown","metadata":{"id":"Q4gemxU6eYk-"},"source":["# Transformers pour la classification de texte\n","\n","L'objectif de ce TP est d'implémenter une version simplifiée d'un Transformer pour résoudre un problème de classification de texte.\n","\n","Nous utiliserons comme exemple illustratif une base de données présente dans la librairie ```Keras``` consistant en des critiques de films postées sur le site IMDB, accompagnées d'une note qui a été binarisée pour révéler le caractèe positif, ou négatif, de la critique."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ypGxrmyTeYlE","executionInfo":{"status":"ok","timestamp":1697706931837,"user_tz":-120,"elapsed":3720,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"1jBnJa2VeYlF"},"source":["## Implémentation d'un bloc de base de Transformer\n","\n","\n","<center><img src=\"https://drive.google.com/uc?id=1leAVyTZJ2gZ26CFoauMtmu7T4Jo-Crc_\" width=200> </center>\n","<caption><center> Figure 1: Schéma de l'architecture de BERT</center></caption>\n","\n","La figure ci-dessus présente l'architecture de BERT. Le bloc de base d'un Transformer est composé d'un bloc de *Self-Attention*, d'une couche de ```Layer Normalization``` (similaire à la ```Batch Normalization```), d'une couche dense et enfin d'une nouvelle couche de ```Layer Normalization```.\n","\n","Pour implémenter la *Self-Attention*, vous pouvez utiliser la fonction ```Multi-Head Attention``` (à vous de regarder quels en sont les paramètres dans la documentation).\n","\n","**Rappel**: Une couche d'Attention *Multi-Head*  se présente sous la forme ci-dessous à gauche, avec le mécanisme d'attention détaillé à droite :\n","\n","\n","<center>\n","\n","<img src=\"https://drive.google.com/uc?id=1UTozEHtsZ3xy61XJqn_Eug-7mn7bFp9m\">\n","<img src=\"https://drive.google.com/uc?id=1aTttpp1OOasVVZAi3lWwosh68VnBjQnz\">\n","</center>\n","\n","D'après vous, combien de paramètres comporte une couche d'attention à 2 têtes, pour un *Embedding* de dimension 32 ?"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VuK3YnbLeYlF","executionInfo":{"status":"ok","timestamp":1697707019754,"user_tz":-120,"elapsed":8,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}}},"outputs":[],"source":["class TransformerBlock(layers.Layer):\n","    # embed_dim désigne la dimension des embeddings maintenus à travers les différentes couches,\n","    # et num_heads le nombre de têtes de la couche d'attention.\n","    def __init__(self, embed_dim, num_heads):\n","        super().__init__()\n","        # Définition des différentes couches qui composent le bloc\n","        # Couche d'attention\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        # Première couche de Layer Normalization\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        # Couche Dense (Feed-Forward)\n","        self.ffn = layers.Dense(units=embed_dim, activation='relu')\n","        # Deuxième couche de normalisation\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","    def call(self, inputs, training):\n","        # Application des couches successives aux entrées\n","        # Couche d'attention\n","        x = self.att(inputs, inputs)\n","        # Ajout de la connexion résiduelle et normalisation\n","        x = self.layernorm1(inputs + x)\n","\n","        # Couche Dense (Feed-Forward)\n","        ffn_output = self.ffn(x)\n","        # Ajout de la connexion résiduelle et normalisation\n","        x = self.layernorm2(x + ffn_output)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"OA9aXUkleYlG"},"source":["## Implémentation de la double couche d'Embedding\n","\n","La séquence d'entrée est convertie en *Embedding* de dimension ```embed_dim```.\n","L'*Embedding* final est constitué de la somme de deux *Embedding*, le premier encodant un mot, et le second encodant la position du mot dans la séquence.\n","\n","La couche d'*Embedding* de Keras (```layers.Embedding```) est une sorte de table associant à un indice en entrée un vecteur de dimension ```embed_dim```. Chaque coefficient de cette table est en fait un paramètre apprenable.\n","\n","D'après vous combien de paramètres contiendrait une couche d'*Embedding* associant un vecteur de dimension 32 à chacun des 20000 mots les plus courants du vocabulaire extrait de la base de données que nous allons utiliser ?\n","Et combien pour l'*Embedding* qui associe un vecteur de dimension 32 à chaque position d'un séquence de longueur ```maxlen``` ?"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RPGCy1t8eYlG","executionInfo":{"status":"ok","timestamp":1697706931838,"user_tz":-120,"elapsed":7,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}}},"outputs":[],"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        # Définition des différentes couches qui composent le bloc Embedding\n","        # Embedding de mot\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","\n","        # Embedding de position\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        # Calcul de l'embedding à partir de l'entrée x\n","        # ATTENTION : UTILISER UNIQUEMENT DES FONCTIONS TF POUR CETTE PARTIE\n","        # Récupération de la longueur de la séquence\n","        maxlen = tf.shape(x)[-1]\n","        # Création d'un vecteur [0, 1, ..., maxlen] des positions associées aux\n","        # mots de la séquence\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        # Calcul des embeddings de position\n","        positions_emb = self.pos_emb(positions)\n","        # Calcul des embeddings de mot\n","        words_emb = self.token_emb(x)\n","        # Somme des embeddings de position et de mot pour obtenir l'embedding final\n","        x = words_emb + positions_emb\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"4ijFa-H_eYlG"},"source":["## Préparation de la base de données"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3EciJaw4eYlG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697706938851,"user_tz":-120,"elapsed":7019,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"c208f59a-5c86-428a-d46c-f1e2717952c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["25000 séquences d'apprentissage\n","25000 séquences de validation\n"]}],"source":["# Taille du vocabulaire considéré (on ne conserve que les 20000 mots les plus courants)\n","vocab_size = 20000\n","# Taille maximale de la séquence considérée (on ne conserve que les 200 premiers mots de chaque commentaire)\n","maxlen = 200\n","\n","# Chargement des données de la base IMDB\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","\n","print(len(x_train), \"séquences d'apprentissage\")\n","print(len(x_val), \"séquences de validation\")\n","\n","# Padding des séquences : ajout de \"0\" pour compléter les séquences trop courtes\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"]},{"cell_type":"markdown","metadata":{"id":"7CkmQNEReYlH"},"source":["## Création du modèle\n","\n","Pour assembler le modèle final, il faut, partant d'une séquence de longueur ```maxlen```, calculer les Embedding puis les fournir en entrée d'une série de blocs Transformer. Pour ce TP, **commencez par ne mettre qu'un seul bloc Transformer**. Vous pourrez en ajouter plus tard si vous le souhaitez.\n","\n","Pour construire la tête de projection du réseau, vous pouvez moyenner les activations en sortie du bloc Transformer par élément de la séquence grâce à un *Global Average Pooling* (1D !), à relier à une couche dense (par exemple comportant 20 neurones) et enfin à la couche de sortie du réseau."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"FiRdlO_SeYlH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697708528235,"user_tz":-120,"elapsed":536,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"3d005fcd-e222-43b7-c8a8-0cc6342af502"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 200)]             0         \n","                                                                 \n"," token_and_position_embeddi  (None, 200, 32)           646400    \n"," ng_2 (TokenAndPositionEmbe                                      \n"," dding)                                                          \n","                                                                 \n"," transformer_block_2 (Trans  (None, 200, 32)           9600      \n"," formerBlock)                                                    \n","                                                                 \n"," global_average_pooling1d_1  (None, 32)                0         \n","  (GlobalAveragePooling1D)                                       \n","                                                                 \n"," dense_4 (Dense)             (None, 20)                660       \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 21        \n","                                                                 \n","=================================================================\n","Total params: 656681 (2.51 MB)\n","Trainable params: 656681 (2.51 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["embed_dim = 32  # Dimension de l'embedding pour chaque mot\n","num_heads = 2  # Nombre de têtes d'attention\n","\n","# A COMPLETER\n","# Création du modèle\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads)\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dense(20, activation='relu')(x)\n","outputs = layers.Dense(1, activation='sigmoid')(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","model.summary()"]},{"cell_type":"markdown","source":["Enfin vous pouvez lancer l'apprentissage, avec par exemple l'optimiseur Adam. Inutile de lancer de trop nombreuses *epochs*, le réseau sur-apprend très vite !"],"metadata":{"id":"6TZq2_8kopTL"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"rfeEvasteYlH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697707218655,"user_tz":-120,"elapsed":157044,"user":{"displayName":"Héloïse Lafargue","userId":"13396784608013268140"}},"outputId":"a45a5b76-5b33-4a82-bff3-ebb056d5b184"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","782/782 [==============================] - 86s 99ms/step - loss: 0.3666 - accuracy: 0.8303 - val_loss: 0.3701 - val_accuracy: 0.8345\n","Epoch 2/5\n","782/782 [==============================] - 25s 31ms/step - loss: 0.1913 - accuracy: 0.9266 - val_loss: 0.3771 - val_accuracy: 0.8535\n","Epoch 3/5\n","782/782 [==============================] - 17s 22ms/step - loss: 0.1279 - accuracy: 0.9545 - val_loss: 0.3575 - val_accuracy: 0.8634\n","Epoch 4/5\n","782/782 [==============================] - 15s 19ms/step - loss: 0.0821 - accuracy: 0.9728 - val_loss: 0.4913 - val_accuracy: 0.8514\n","Epoch 5/5\n","782/782 [==============================] - 13s 17ms/step - loss: 0.0563 - accuracy: 0.9816 - val_loss: 0.5575 - val_accuracy: 0.8494\n"]}],"source":["# A COMPLETER\n","model.compile(\n","    optimizer=\"adam\",\n","    loss=\"binary_crossentropy\",\n","    metrics=[\"accuracy\"]\n",")\n","\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val)\n",")"]},{"cell_type":"markdown","source":["**Questions subsidiaires**:\n","\n","\n","\n","1.   Testez un LSTM bi-directionnel, comme nous l'avons vu dans le TP précédent, et comparez les résultats obtenus sur ce problème.\n","\n"],"metadata":{"id":"O5XtVsJLo5h2"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}